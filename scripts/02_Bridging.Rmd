---
title: "Bridging"
date: "2025-06-25"
output: 
  html_document:
    toc: true
    toc_float:
      collapsed: true
    toc_depth: 3
    number_sections: false
    theme: lumen
---

# Background

Our previous results of analyzing the discovery and validation dataset seperately were inconsistent The DE genes were completely different. Here we want to join / bridge the data from different batches.

# Objectives

1. Individual analysis
2. Limma correction
3. Olink correction
4. Sample number plot

# Conclusion

We analyzed individual proteomics dataset and removed outliers (samples and proteins). Using the overlapping proteins, we applied batch correction using the Limmad batch correction methods. We performed Olink default correction methods and the resutl was sub-optimal (sampels were still clustering by batch). So we focus on Limma correction. 

# Pre-processing 

## Loading packages

```{r, message=FALSE, warning=FALSE}
library(here)
library(tidyverse)
library(readxl)   
library(readr)
library(stringr)
library(limma)
library(variancePartition)

library(tidyverse)
library(ggplot2)
library(patchwork)
library(ggrepel)
library(gghighlight)
library(ggbeeswarm)
library(VennDiagram)
library(ggvenn)


library(OlinkAnalyze) 
library(paletteer)

# Survival
library(survival)
library(ggsurvfit)
library(survminer)
```

## Directories

```{r}
wd <- list()
wd$main <- here()
wd$data <- file.path(wd$main, "data")
wd$manishData <- file.path(wd$data, "raw/data_fromManish")
wd$d2024 <- file.path(wd$data, "updated_2024")
wd$d2024_npx <- file.path(wd$d2024, "NPX_data")
wd$output <- file.path(wd$main, "output")
wd$script <- file.path(wd$main, "scripts")

wd$outData <- file.path(wd$output, "data")
wd$outCurr <- file.path(wd$output, "02_Bridging")

set.seed(123)
```


Create directories

```{r}
if (!file.exists(wd$outCurr)) {
  dir.create(wd$outCurr)
} else {
  print("Directory already exists")
}
```

Load functions

```{r}
source(file.path(wd$script, "functions.R"))
```

## Load data

Load the processed metadata file

```{r}
load(file.path(wd$outData, "01_Metadata.Rdata"))

# Contains the sample IDs
df_meta_f <- df_meta_f4
```


# Obj 1: Individual analysis

Here we want to do exploratory analysis, filtering of samples and proteins on the dataset individually. We would like to remove poor quality samples / proteins. 

## Psomagen

We load the NPX and the psomagen metadata 

```{r}
# dat_NPX - contains the NPX of the Psomagen cohort

df_meta <- df_meta_f %>%   
  # Get all the psomagen samples
  filter(Psomagen == "Y") %>% 
  filter(!is.na(sample_id_psom)) %>% 
  select(-txt_stat) %>% 
  dplyr::rename(SampleID = sample_id_psom,
                txt_stat = txt_stat_ordered) %>% 
  select(mrn, cohort, SampleID, txt_stat) %>% 
  distinct() %>% 
  mutate(SampleID = as.character(SampleID))

# This removes control samples as well
dat1_NPX_anno <- dat_NPX %>%
  mutate(SampleID = as.character(SampleID)) %>% 
  dplyr::left_join(df_meta) %>%
  filter(!is.na(cohort))
```

### Quick PCA 

```{r}
p_pca <-
  dat1_NPX_anno %>% 
  filter(!str_detect(SampleID, 'CS')) %>% 
  olink_pca_plot(df = .,
                 color_g = "cohort", byPanel = FALSE, quiet = FALSE) 

pca_cor <- p_pca[[1]]$data %>%
  rename(PC1 = PCX,
         PC2 = PCY) %>% 
  #Combine with master annotation
  left_join(df_meta)

ggplot(pca_cor, aes(x=PC1, y=PC2, colour=cohort)) +
  geom_point(size=2.5) +
  ylab(p_pca[[1]]$labels$y) +
  xlab(p_pca[[1]]$labels$x) +
  theme_classic() +
  theme(legend.position = "right",
        panel.grid.major = element_line(linetype = "blank"), 
        panel.grid.minor = element_line(linetype = "blank")) +
  scale_colour_manual(values=pal.cohort) +
  NULL
```
Get the PCA coordinates

```{r}
pca_cor <- p_pca[[1]]$data %>%
  rename(PC1 = PCX,
         PC2 = PCY) %>% 
  #Combine with master annotation
  left_join(df_meta) 
```

Plot a different color

```{r}
p3 <- 
ggplot(pca_cor, aes(x=PC1, y=PC2, fill=cohort)) +
  geom_point(shape=21, size=3) +
  ylab(p_pca[[1]]$labels$y) +
  xlab(p_pca[[1]]$labels$x) +
  theme_classic() +
  theme(legend.position = "right",
        panel.grid.major = element_line(linetype = "blank"), 
        panel.grid.minor = element_line(linetype = "blank")) +
  scale_fill_manual(values=pal.cohort.n) +
  NULL



ggsave(p3, file=file.path(wd$outCurr, "PCA_discovery.pdf"), height = 5, width = 7)
```

Color based on some other variable

```{r}
pca_cor %>% filter(cohort == "C") %>% 
ggplot(., aes(x=PC1, y=PC2, colour=txt_stat)) +
  geom_point(size=2.5) +
  ylab(p_pca[[1]]$labels$y) +
  xlab(p_pca[[1]]$labels$x) +
  theme_classic() +
  theme(legend.position = "right",
        panel.grid.major = element_line(linetype = "blank"), 
        panel.grid.minor = element_line(linetype = "blank")) +
  NULL
```

### Sample outlier

The Olink explore has 8 panels (https://olink.com/products/olink-explore-3072-384). Here, we check if any samples were considered outlier in all of the 8 panels. 

We use the methods described by Olink (https://cran.r-project.org/web/packages/OlinkAnalyze/vignettes/OutlierExclusion.html). Outliers are defined based on the interquartile range (IQR) range. 

This is grounds for exclusion. 

```{r}
out_median = 4
out_iqr = 4
outliers_qc_labeled <- dat1_NPX_anno |> 
  olink_qc_plot(
    median_outlierDef = out_median, IQR_outlierDef = out_iqr,
    outlierLines = FALSE, label_outliers = TRUE) 

dat_outlier <- outliers_qc_labeled$data

sam_outlier <- dat_outlier %>% group_by(SampleID, Outlier) %>% 
  count() %>% 
  filter(Outlier == 1) %>% 
  arrange(desc(n))
head(sam_outlier)
```

> No sample failed in all 8 panel. So we retain everything

### Assay outlier

We want to check if any proteins needs to be excluded. Olink provides a collumn called "QC_Warning" that we can use to determine. 

```{r}
warn_assay <- dat1_NPX_anno %>% filter(Assay_Warning == "WARN") %>% select(Assay, OlinkID) %>% distinct()
```

Or we can determine what number of samples are expressing the protein above the limit of detection

```{r}
numSample1 <- length(unique(dat1_NPX_anno$SampleID))
  
dat1_missing <- dat1_NPX_anno %>% 
  group_by(OlinkID) %>% 
  summarise(PercMissing = (sum(NPX < LOD) / numSample1) * 100) %>% 
  mutate(Group = "Psomagen")
```

Lets retain all the proteins with less than 80% missing-ness

```{r}
perc_miss = 80
assay_keep <- dat1_missing %>% 
  filter(PercMissing <= perc_miss)  %>% 
  filter(!OlinkID %in% warn_assay$OlinkID)
length(unique(assay_keep$OlinkID))
```

Lets subset the NPX data frame to these proteins

```{r}
dat1_final <- dat1_NPX_anno %>% 
  filter(OlinkID %in% assay_keep$OlinkID)
```

Sanity check

```{r}
length(unique(dat1_NPX_anno$OlinkID))
length(unique(dat1_final$OlinkID))
```

We re-run the PCA to see the distribution

```{r}
dat1_pca <-
  dat1_final %>% 
  filter(!str_detect(SampleID, 'CS')) %>% 
  olink_pca_plot(df = .,
                 color_g = "cohort", byPanel = FALSE, quiet = FALSE) 
```

## Q-1335

Repeat what we did previously (PCA, outlier detection)

```{r}
# dat_NPX2 - contains the NPX of Q-1335
df_meta <- df_meta_f %>%   
  # Get all the psomagen samples
  filter(Q_13356 == "Y") %>% 
  select(-txt_stat) %>% 
  dplyr::rename(SampleID = sample_id_Q13356,
                txt_stat = txt_stat_ordered) %>% 
  select(mrn, cohort, SampleID, txt_stat) %>% 
  distinct() %>% 
  mutate(SampleID = as.character(SampleID))

# This removes conttol samples as well
dat2_NPX_anno <- dat_NPX2 %>%
  mutate(SampleID = as.character(SampleID)) %>% 
  dplyr::left_join(df_meta) %>%
  filter(!is.na(cohort))
```

### Quick PCA

```{r}
p_pca <-
  dat2_NPX_anno %>% 
  filter(!str_detect(SampleID, 'CS')) %>% 
  olink_pca_plot(df = .,
                 color_g = "cohort", byPanel = FALSE, quiet = FALSE) 

pca_cor <- p_pca[[1]]$data %>%
  rename(PC1 = PCX,
         PC2 = PCY) %>% 
  #Combine with master annotation
  left_join(df_meta)

ggplot(pca_cor, aes(x=PC1, y=PC2, colour=cohort)) +
  geom_point(size=2.5) +
  ylab(p_pca[[1]]$labels$y) +
  xlab(p_pca[[1]]$labels$x) +
  theme_classic() +
  theme(legend.position = "right",
        panel.grid.major = element_line(linetype = "blank"), 
        panel.grid.minor = element_line(linetype = "blank")) +
  scale_colour_manual(values=pal.cohort) +
  NULL
```
Get the PCA coordinates

```{r}
pca_cor <- p_pca[[1]]$data %>%
  rename(PC1 = PCX,
         PC2 = PCY) %>% 
  #Combine with master annotation
  left_join(df_meta) 
```

Plot a different color

```{r}
p3 <- 
ggplot(pca_cor, aes(x=PC1, y=PC2, fill=cohort)) +
  geom_point(shape=21, size=3) +
  ylab(p_pca[[1]]$labels$y) +
  xlab(p_pca[[1]]$labels$x) +
  theme_classic() +
  theme(legend.position = "right",
        panel.grid.major = element_line(linetype = "blank"), 
        panel.grid.minor = element_line(linetype = "blank")) +
  scale_fill_manual(values=pal.cohort.n[2:3]) +
  NULL



ggsave(p3, file=file.path(wd$outCurr, "PCA_Q_13356.pdf"), height = 5, width = 7)
```

Color based on some other variable

```{r}
ggplot(pca_cor, aes(x=PC1, y=PC2, colour=txt_stat)) +
  geom_point(size=2.5) +
  ylab(p_pca[[1]]$labels$y) +
  xlab(p_pca[[1]]$labels$x) +
  theme_classic() +
  theme(legend.position = "right",
        panel.grid.major = element_line(linetype = "blank"), 
        panel.grid.minor = element_line(linetype = "blank")) +
  NULL
```
### Sample outlier

Repeat sample outlier analysis

```{r}
outliers_qc_labeled <- dat2_NPX_anno |> 
  olink_qc_plot(
    median_outlierDef = out_median, IQR_outlierDef = out_iqr,
    outlierLines = FALSE, label_outliers = TRUE) 

dat_outlier <- outliers_qc_labeled$data

sam_outlier <- dat_outlier %>% group_by(SampleID, Outlier) %>% 
  count() %>% 
  filter(Outlier == 1) %>% 
  arrange(desc(n))
head(sam_outlier)
```
We exclude two samples that were deemed outlier. One sample was considered outlier in all 8 panels, the other 7/8 panels.

```{r}
all_fail <- sam_outlier %>% filter(n>=7)
dat2_final <- dat2_NPX_anno %>% 
  filter(!SampleID %in% all_fail$SampleID)
```

### Protein outlier

We want to check if any proteins needs to be excluded. Olink provides a collumn called "QC_Warning" that we can use to determine. 

```{r}
warn_assay <- dat2_final %>% filter(Assay_Warning == "WARN") %>% select(Assay, OlinkID) %>% distinct()
```

Or we can determine what number of samples are expressing the protein above the limit of detection

```{r}
numSample1 <- length(unique(dat2_final$SampleID))
  
dat2_missing <- dat2_final %>% 
  group_by(OlinkID) %>% 
  summarise(PercMissing = (sum(NPX < LOD) / numSample1) * 100) %>% 
  mutate(Group = "Q_1335")
```

Lets retain all the proteins with less than 80% missing-ness

```{r}
assay_keep_2 <- dat2_missing %>% 
  filter(PercMissing <= perc_miss)  %>% 
  filter(!OlinkID %in% warn_assay$OlinkID)
length(unique(assay_keep_2$OlinkID))
```

Lets subset the NPX data frame to these proteins

```{r}
dat2_final <- dat2_final %>% 
  filter(OlinkID %in% assay_keep_2$OlinkID)
```

## Q-15806

Repeat fo the final batch. 

```{r}
df_meta <- df_meta_f %>%   
  filter(Q_15806 == "Y") %>% 
  select(-txt_stat) %>% 
  dplyr::rename(SampleID = sample_id_Q15806,
                txt_stat = txt_stat_ordered) %>% 
  select(mrn, cohort, SampleID, txt_stat) %>% 
  distinct() %>% 
  mutate(SampleID = as.character(SampleID))

# This removes control samples as well
dat3_NPX_anno <- dat_NPX3 %>%
  mutate(SampleID = as.character(SampleID)) %>% 
  dplyr::left_join(df_meta) %>%
  filter(!is.na(cohort))
```

### Quick PCA

```{r}
p_pca <-
  dat3_NPX_anno %>% 
  filter(!str_detect(SampleID, 'CS')) %>% 
  olink_pca_plot(df = .,
                 color_g = "cohort", byPanel = FALSE, quiet = FALSE) 

pca_cor <- p_pca[[1]]$data %>%
  rename(PC1 = PCX,
         PC2 = PCY) %>% 
  #Combine with master annotation
  left_join(df_meta)

ggplot(pca_cor, aes(x=PC1, y=PC2, colour=cohort)) +
  geom_point(size=2.5) +
  ylab(p_pca[[1]]$labels$y) +
  xlab(p_pca[[1]]$labels$x) +
  theme_classic() +
  theme(legend.position = "right",
        panel.grid.major = element_line(linetype = "blank"), 
        panel.grid.minor = element_line(linetype = "blank")) +
  scale_colour_manual(values=pal.cohort) +
  NULL
```
Get the PCA coordinates

```{r}
pca_cor <- p_pca[[1]]$data %>%
  rename(PC1 = PCX,
         PC2 = PCY) %>% 
  #Combine with master annotation
  left_join(df_meta) 
```

Plot a different color

```{r}
p3 <- 
ggplot(pca_cor, aes(x=PC1, y=PC2, fill=cohort)) +
  geom_point(shape=21, size=3) +
  ylab(p_pca[[1]]$labels$y) +
  xlab(p_pca[[1]]$labels$x) +
  theme_classic() +
  theme(legend.position = "right",
        panel.grid.major = element_line(linetype = "blank"), 
        panel.grid.minor = element_line(linetype = "blank")) +
  scale_fill_manual(values=pal.cohort.n) +
  NULL



ggsave(p3, file=file.path(wd$outCurr, "PCA_Q_1580.pdf"), height = 5, width = 7)
```

Color based on some other variable

```{r}
ggplot(pca_cor, aes(x=PC1, y=PC2, colour=txt_stat)) +
  geom_point(size=2.5) +
  ylab(p_pca[[1]]$labels$y) +
  xlab(p_pca[[1]]$labels$x) +
  theme_classic() +
  theme(legend.position = "right",
        panel.grid.major = element_line(linetype = "blank"), 
        panel.grid.minor = element_line(linetype = "blank")) +
  NULL
```


### Sample outlier

Repeat sample outlier analysis

```{r}
outliers_qc_labeled <- dat3_NPX_anno |> 
  olink_qc_plot(
    median_outlierDef = out_median, IQR_outlierDef = out_iqr,
    outlierLines = FALSE, label_outliers = TRUE) 

dat_outlier <- outliers_qc_labeled$data

sam_outlier <- dat_outlier %>% group_by(SampleID, Outlier) %>% 
  count() %>% 
  filter(Outlier == 1) %>% 
  arrange(desc(n))
head(sam_outlier)
```

We exclude two samples that were deemed outlier in all 6 panels. We also exclude the sampleID 2, because its the only one that came up. 

```{r}
all_fail <- sam_outlier %>% filter(n >= 1)
dat3_final <- dat3_NPX_anno %>% 
  filter(!SampleID %in% all_fail$SampleID)
```

### Protein outlier

We want to check if any proteins needs to be excluded. Olink provides a collumn called "QC_Warning" that we can use to determine. 

```{r}
warn_assay <- dat3_final %>% filter(Assay_Warning == "WARN") %>% select(Assay, OlinkID) %>% distinct()
```

Or we can determine what number of samples are expressing the protein above the limit of detection

```{r}
numSample1 <- length(unique(dat3_final$SampleID))
  
dat3_missing <- dat3_final %>% 
  group_by(OlinkID) %>% 
  summarise(PercMissing = (sum(NPX < LOD) / numSample1) * 100) %>% 
  mutate(Group = "Q_1580")
```

Lets retain all the proteins with less than 80% missing-ness

```{r}
assay_keep_3 <- dat3_missing %>% 
  filter(PercMissing <= perc_miss)  %>% 
  filter(!OlinkID %in% warn_assay$OlinkID)
length(unique(assay_keep_3$OlinkID))
```

Lets subset the NPX data frame to these proteins

```{r}
dat3_final <- dat3_final %>% 
  filter(OlinkID %in% assay_keep_3$OlinkID)
```

### Re-run PCA

We re-run the PCA to see the distribution

```{r}
dat3_pca <-
  dat3_final %>% 
  filter(!str_detect(SampleID, 'CS')) %>% 
  olink_pca_plot(df = .,
                 color_g = "cohort", byPanel = FALSE, quiet = FALSE) 
```

We see an extreme plot of 2 samples from cohort A. Lets explore further.

Lets gather all cohort A samples, then check the NPX distribution

```{r}
dat3_final_A <- dat3_final %>% filter(cohort == "A")

# Get the PCA coordinate to identify these sanmples
pca_cor_bridge <- dat3_pca[[1]]$data %>%
  rename(PC1 = PCX,
         PC2 = PCY) %>% 
  mutate(outlier = case_when(
    colors == "A" & PC1 >= 0.1 ~ "Outlier",
    TRUE ~ "Keep"
  ))
rmv_sam <- pca_cor_bridge %>% filter(outlier == "Outlier") %>% pull(SampleID)
rmv_sam

# Label 
dat3_final_A <- dat3_final_A %>% mutate(outlier = case_when(SampleID %in% rmv_sam ~ "Outlier",
                                                                TRUE ~ "Not"))

# Check NPX distibution
dat3_final_A |> 
  olink_dist_plot(color_g = "outlier")
```
These 2 samples have consistently higher NPX compared to other samples. So we remove them. 

```{r}
dat3_final <- dat3_final %>% 
  filter(!SampleID %in% rmv_sam)
```

Re-plot the PCA

```{r}
dat3_pca_v2 <-
  dat3_final %>% 
  filter(!str_detect(SampleID, 'CS')) %>% 
  olink_pca_plot(df = .,
                 color_g = "cohort", byPanel = FALSE, quiet = FALSE) 
```

## Protein overlap

Lets get the overlap of proteins across 3 platforms

```{r}
un_a <- unique(dat1_final$OlinkID)
un_b <- unique(dat2_final$OlinkID)
un_c <- unique(dat3_final$OlinkID)

overlap <- calculate.overlap(
x <- list("Psomagen"=un_a, 
          "Q-1335"=un_b,
          "Q-1580"=un_c))

pdf(file=file.path(wd$outCurr, "Protein_overlap.pdf"), width = 5, height = 5)
ggvenn(
  x, 
  fill_color = df.pal.study$pal,
  show_percentage = FALSE,
  stroke_size = 0.5, set_name_size = 4
)
dev.off()

ggvenn(
  x, 
  fill_color = pal.study,
  show_percentage = FALSE,
  stroke_size = 0.5, set_name_size = 4
)
```

Save this list of proteins

```{r}
# Get all unique proteins across all datasets
all_proteins <- df_id_pro$OlinkID

# Create presence columns
protein_presence <- data.frame(
  OlinkID = all_proteins,
  In_psom = as.integer(all_proteins %in% un_a),
  In_q1335 = as.integer(all_proteins %in% un_b),
  In_q1580 = as.integer(all_proteins %in% un_c)
)

# Annotate with protein names
protein_presence <- protein_presence %>%  left_join(df_id_pro)

# Save to file
write.csv(protein_presence, file.path(wd$outCurr, "Protein_presence_table.csv"), row.names = FALSE)
```

> We filtered the samples and proteins. We will bridge the data using these proteins

# Obj 2: Limma correction

We explore limma to integrate these data

### Data prep

```{r}
xx1 <- dat1_final
xx2 <- dat2_final
xx3 <- dat3_final
```

Convert to gene x matrix

```{r}
mat_olink1 <- xx1 %>%
  dplyr::select(SampleID, OlinkID, NPX) %>%
  # Remove contraol samples
  dplyr::filter(!str_detect(SampleID, 'CONTROL_SAMPLE')) %>% 
  # Convert to wide
  tidyr::pivot_wider(names_from = SampleID, values_from = NPX)
colnames(mat_olink1)[2:ncol(mat_olink1)] <- paste0("Disc_", colnames(mat_olink1)[2:ncol(mat_olink1)] )

mat_olink2 <- xx2 %>%
  dplyr::select(SampleID, OlinkID, NPX) %>%
  # Remove contraol samples
  dplyr::filter(!str_detect(SampleID, 'CONTROL_SAMPLE')) %>% 
  # Convert to wide
  tidyr::pivot_wider(names_from = SampleID, values_from = NPX)
colnames(mat_olink2)[2:ncol(mat_olink2)] <- paste0("Proj2_", colnames(mat_olink2)[2:ncol(mat_olink2)] )

mat_olink3 <- xx3 %>%
  dplyr::select(SampleID, OlinkID, NPX) %>%
  # Remove contraol samples
  dplyr::filter(!str_detect(SampleID, 'CONTROL_SAMPLE')) %>% 
  # Convert to wide
  tidyr::pivot_wider(names_from = SampleID, values_from = NPX)
colnames(mat_olink3)[2:ncol(mat_olink3)] <- paste0("Proj4_", colnames(mat_olink3)[2:ncol(mat_olink3)] )
```

Selecting the genes

```{r}
# Create a matrix of just overlapping genes
int_ol <-
  Reduce(intersect, list(
  mat_olink1$OlinkID,
  mat_olink2$OlinkID,
  mat_olink3$OlinkID))

m1 <- mat_olink1 %>% as.data.frame()
rownames(m1) <- m1$OlinkID
m1 <- m1[int_ol,-1]

m2 <- mat_olink2 %>% as.data.frame()
rownames(m2) <- m2$OlinkID
m2 <- m2[int_ol,-1]

m3 <- mat_olink3 %>% as.data.frame()
rownames(m3) <- m3$OlinkID
m3 <- m3[int_ol,-1]


m_all <- cbind(m1, m2, m3)
```

### Running limma

Here we use limma to correct the batch effects. First we create a data frame to indicate the replicates across experiments

```{r, eval=TRUE}
# Define the model formula
# Generate master replicate sheets
vector1 <- df_meta_f %>% filter(Psomagen == "Y") %>% pull(HCI_cID)
vector2 <- df_meta_f %>% filter(Q_13356 == "Y") %>% pull(HCI_cID)
vector3 <- df_meta_f %>% filter(Q_15806 == "Y") %>% pull(HCI_cID)

# Intersection of any two vectors
intersection_any <- union(intersect(vector1, vector2), union(intersect(vector1, vector3), intersect(vector2, vector3)))

# Bridge data
df_bridge <- df_meta_f %>% 
  filter(HCI_cID %in% intersection_any) %>% 
  select(HCI_cID, cohort, sample_id_psom, sample_id_Q13356, sample_id_Q15806) %>% 
  mutate(Rep = as.character( sprintf("%02d" ,1:length(intersection_any))))

df_bridge2 <- data.frame(
  SampleID = 
    c(paste(paste0("Disc_", df_bridge$sample_id_psom)),
      paste(paste0("Proj2_", df_bridge$sample_id_Q13356)),
      paste(paste0("Proj4_", df_bridge$sample_id_Q15806))),
  Rep = paste0("Rep_", df_bridge$Rep)) %>% 
  filter(!str_detect(SampleID, "_NA"))

meta_data <- data.frame(
  SampleID = colnames(m_all),
  Batch = c(
    rep("Dis", each=ncol(m1)),
    rep("Proj_2", each=ncol(m2)),
    rep("Proj_4", each=ncol(m3))
)) %>% left_join(df_bridge2) %>% 
  # Create replicate IDs
  # For technical replicates, use the same ID across runs
  mutate(ReplicateID = case_when(!is.na(Rep) ~ Rep,
                                 TRUE ~ SampleID))
head(meta_data)
meta_data %>% filter(Rep == "Rep_15")

# Add cohort info
df_bridge <- df_meta_f %>% 
  select(HCI_cID, cohort, sample_id_psom, sample_id_Q13356, sample_id_Q15806)

df_bridge2 <- data.frame(
  SampleID = 
    c(paste(paste0("Disc_", df_bridge$sample_id_psom)),
      paste(paste0("Proj2_", df_bridge$sample_id_Q13356)),
      paste(paste0("Proj4_", df_bridge$sample_id_Q15806))),
  cohort = df_bridge$cohort,
  HCI_cID = df_bridge$HCI_cID) %>% 
  filter(!str_detect(SampleID, "_NA"))

meta_data <- meta_data %>% left_join(df_bridge2)
#colnames(m_all) <- meta_data$ReplicateID
# Ensure that SampleID matches the columns in counts
stopifnot(all(meta_data$SampleID == colnames(m_all)))

# Ensure the samples are in the same order
meta_data <- meta_data[match(colnames(m_all), meta_data$SampleID), ]
# Verify the order matches
all(colnames(m_all) == meta_data$SampleID)  # Should return TRUE
rownames(meta_data) <- meta_data$SampleID

meta_data <- meta_data %>% 
  mutate(cohort = factor(cohort),
         Batch = factor(Batch),
         ReplicateID = factor(ReplicateID))
```

Now lets perform the batch correction

```{r}
# Set up parallel processing (optional)
param <- SnowParam(4, "SOCK")  # Adjust the number of cores based on your system

design <- model.matrix(~ cohort, data = meta_data)

# Estimate within-replicate correlation
corfit <- duplicateCorrelation(m_all, design, block = meta_data$ReplicateID)

# Remove batch effects
exprMatrix_corrected <- removeBatchEffect(
  m_all,
  batch = meta_data$Batch,
  block = meta_data$ReplicateID,
  correlation = corfit$consensus
)
```

Vizualise the result before and after correlation

```{r}
pt_sz <- 2
# PCA before
# PCA before batch correction
pca_before <- prcomp(t(m_all), scale. = TRUE)
meta_data$PC1_before <- pca_before$x[,1]
meta_data$PC2_before <- pca_before$x[,2]

# Calculate variance explained
pc_variances <- pca_before$sdev^2
variance_explained <- pc_variances / sum(pc_variances)
percent_variance_explained <- variance_explained * 100

# Extract variance for PC1 and PC2
pc1_variance <- percent_variance_explained[1]
pc2_variance <- percent_variance_explained[2]

# Create labels for plotting
pc1_label <- paste0("PC1 (", round(pc1_variance, 2), "%)")
pc2_label <- paste0("PC2 (", round(pc2_variance, 2), "%)")


p1.1 <- 
ggplot(meta_data, aes(x = PC1_before, y = PC2_before, color = Batch)) +
  geom_point(size = pt_sz) +
  labs(
    title = "PCA Before Batch Correction",
    x = pc1_label,
    y = pc2_label) +
  theme_bw() +
  theme_custom +
  theme(legend.position = "none") +
  scale_colour_manual(values = pal.study) +
  NULL


p1.2 <- 
ggplot(meta_data, aes(x = PC1_before, y = PC2_before, color = cohort)) +
  geom_point(size = pt_sz) +
  labs(
    title = "PCA Before Batch Correction",
    x = pc1_label,
    y = pc2_label) +
  theme_bw() +
  theme_custom +
  theme(legend.position = "none") +
  scale_colour_manual(values = pal.cohort.n2) +
  NULL
```

## Correlation plot

Get the correlation of all the samples. 

```{r}
# Reshape the gene expression data for plotting
long_expr <- exprMatrix_corrected %>% as.data.frame() %>% 
  rownames_to_column("Gene") %>%
  pivot_longer(cols = -Gene, names_to = "SampleID", values_to = "Expression")
```

Match replicates based on the replicate mapping

```{r}
replicate_pairs <- meta_data %>%
  filter(!is.na(Rep)) %>%
  inner_join(meta_data, by = "Rep") %>%
  filter(SampleID.x != SampleID.y) %>%
  select(Rep, SampleID.x, SampleID.y) %>%
  distinct()

dplyr::filter(replicate_pairs, SampleID.y == "Proj2_4")
```

Loop to get the correlation values

> Run this just once

```{r,eval=FALSE}
# Cor-pair for each replicate
# Create an empty data frame to store correlation results
# Create an empty data frame to store correlation results
cor_results <- data.frame(
  Sample1 = character(),
  Sample2 = character(),
  Cohort1 = character(),
  Cohort2 = character(),
  Correlation = numeric(),
  Rep = character(),
  stringsAsFactors = FALSE
)

# Get all unique pairs of samples
sample_pairs <- expand.grid(SampleID_x = unique(long_expr$SampleID),
                            SampleID_y = unique(long_expr$SampleID),
                            stringsAsFactors = FALSE) %>%
  filter(SampleID_x < SampleID_y)  # Avoid duplicate pairs and self-comparisons

# Compute correlation for each pair
cor_results <- sample_pairs %>%
  rowwise() %>%
  mutate(
    cor_value = {
      df <- inner_join(
        long_expr %>% filter(SampleID == SampleID_x) %>% rename(Expression_x = Expression),
        long_expr %>% filter(SampleID == SampleID_y) %>% rename(Expression_y = Expression),
        by = "Gene"
      )
      
      cor(df$Expression_x, df$Expression_y, use = "pairwise.complete.obs")
    },
    Cohort1 = meta_data$cohort[meta_data$SampleID == SampleID_x],
    Cohort2 = meta_data$cohort[meta_data$SampleID == SampleID_y],
    Rep = ifelse(
      meta_data$Rep[meta_data$SampleID == SampleID_x] == meta_data$Rep[meta_data$SampleID == SampleID_y] & 
        !is.na(meta_data$Rep[meta_data$SampleID == SampleID_x]), 
      meta_data$Rep[meta_data$SampleID == SampleID_x], 
      NA
    )
  ) %>%
  select(Sample1 = SampleID_x, Sample2 = SampleID_y, Cohort1, Cohort2, Correlation = cor_value, Rep)

```

We examine the correlation results. 

```{r}
#write.csv(cor_results, file.path(wd$outCurr, "Filtered_genes_LimmaCorrected_cor.csv"))
#cor_results_ori <- cor_results
cor_results <- read.csv(file.path(wd$outCurr, "Filtered_genes_LimmaCorrected_cor.csv"))
```

We saw some correlations are below 0.6, we remove these replicates as the correlation is rather low. 

```{r}
df_low <- cor_results %>% filter(!is.na(Rep)) %>% filter(Correlation <= 0.5)
tail(df_low)
```

Identify the samples to be removed after correlation analysis

```{r}
df_low_sample <- unique(df_low$Sample2)
```

Re-run the entire workflow

```{r}
# Filter the meta data to remove the poor correlated samples
meta_data2 <- meta_data %>% filter(!SampleID %in% df_low_sample) 

# Get the number of overlapping samples
rep_1 <- filter(meta_data2, Batch=="Dis") %>% pull(ReplicateID)
rep_2 <- filter(meta_data2, Batch=="Proj_2") %>% pull(ReplicateID)
rep_3 <- filter(meta_data2, Batch=="Proj_4") %>% pull(ReplicateID)


overlap <- calculate.overlap(
  x <- list(
    "Dis"=rep_1,
    "Q_13356"=rep_2, 
    "Q_15806"=rep_3))

ggvenn(
  x, 
  fill_color = pal.study,
  show_percentage = FALSE,
  stroke_size = 0.5, set_name_size = 4
)
```

Run the correlation analysis

```{r}
# Remove the samples from the matrix
m_all2 <- m_all[,meta_data2$SampleID]

# Set up parallel processing (optional)
param <- SnowParam(4, "SOCK")  # Adjust the number of cores based on your system

design <- model.matrix(~ cohort, data = meta_data2)

# Estimate within-replicate correlation
corfit <- duplicateCorrelation(m_all2, design, block = meta_data2$ReplicateID)

# Remove batch effects
exprMatrix_corrected2 <- removeBatchEffect(
  m_all2,
  batch = meta_data2$Batch,
  block = meta_data2$ReplicateID,
  correlation = corfit$consensus
)
```

Lets re-do the correlation again, limited to those with Rep

```{r}
# Reshape the gene expression data for plotting
long_expr <- exprMatrix_corrected2 %>% as.data.frame() %>% 
  rownames_to_column("Gene") %>%
  pivot_longer(cols = -Gene, names_to = "SampleID", values_to = "Expression")
```

Match replicates based on the replicate mapping

```{r}
replicate_pairs <- meta_data2 %>%
  filter(!is.na(Rep)) %>%
  inner_join(meta_data, by = "Rep") %>%
  filter(SampleID.x != SampleID.y) %>%
  select(Rep, SampleID.x, SampleID.y) %>%
  distinct()
```

```{r}
long_expr <- long_expr %>% filter(SampleID %in% 
                                    unique(c(replicate_pairs$SampleID.x, replicate_pairs$SampleID.y)))

# Cor-pair for each replicate
cor_results <- data.frame(
  Sample1 = character(),
  Sample2 = character(),
  Cohort1 = character(),
  Cohort2 = character(),
  Correlation = numeric(),
  Rep = character(),
  stringsAsFactors = FALSE
)

# Get all unique pairs of samples
sample_pairs <- expand.grid(SampleID_x = unique(long_expr$SampleID),
                            SampleID_y = unique(long_expr$SampleID),
                            stringsAsFactors = FALSE) %>%
  filter(SampleID_x < SampleID_y)  # Avoid duplicate pairs and self-comparisons

# Compute correlation for each pair
cor_results <- sample_pairs %>%
  rowwise() %>%
  mutate(
    cor_value = {
      df <- inner_join(
        long_expr %>% filter(SampleID == SampleID_x) %>% rename(Expression_x = Expression),
        long_expr %>% filter(SampleID == SampleID_y) %>% rename(Expression_y = Expression),
        by = "Gene"
      )
      
      cor(df$Expression_x, df$Expression_y, use = "pairwise.complete.obs")
    },
    Cohort1 = meta_data$cohort[meta_data$SampleID == SampleID_x],
    Cohort2 = meta_data$cohort[meta_data$SampleID == SampleID_y],
    Rep = ifelse(
      meta_data$Rep[meta_data$SampleID == SampleID_x] == meta_data$Rep[meta_data$SampleID == SampleID_y] & 
        !is.na(meta_data$Rep[meta_data$SampleID == SampleID_x]), 
      meta_data$Rep[meta_data$SampleID == SampleID_x], 
      NA
    )
  ) %>%
  select(Sample1 = SampleID_x, Sample2 = SampleID_y, Cohort1, Cohort2, Correlation = cor_value, Rep)

```

Save the results

```{r}
write.csv(cor_results, file.path(wd$outCurr, "Filtered_genes_LimmaCorrected_removedLow_cor.csv"))
```

> Lets keep this workflow, removing the poor correlated samples (less then 0.5)

## Remove dupe

We can see that technical replicates are kept. 

For example

```{r}
nrow(meta_data2)
```

We get 342, but we actually just have `nrow(df_meta_f)` samples. So we need to remove technical duplicates. 
So for the samples with repeated measurement, we want to just keep 1. 

For those profiled multiple times. We will keep the Psomagen data first, then the Q15806, then the Q 13356.

```{r}
df_unq <- df_meta_f %>%
  mutate(
    sample_id_psom = paste0("Disc_", sample_id_psom),
    sample_id_psom = case_when(sample_id_psom == "Disc_NA" ~ NA, TRUE ~ sample_id_psom),
    sample_id_Q13356 = paste0("Proj2_", sample_id_Q13356),
    sample_id_Q13356 = case_when(sample_id_Q13356 == "Proj2_NA" ~ NA, TRUE ~ sample_id_Q13356),
    sample_id_Q15806 = paste0("Proj4_", sample_id_Q15806),
    sample_id_Q15806 = case_when(sample_id_Q15806 == "Proj4_NA" ~ NA, TRUE ~ sample_id_Q15806),
    final_sample_id = coalesce(sample_id_psom, sample_id_Q13356, sample_id_Q15806))
length(unique(df_unq$final_sample_id))
df_final_ids <- df_unq
```

Lets subset to the samples of interest

```{r}
# Important - select the variable after removing the poor correlations
df_tmp <- df_unq %>% filter(final_sample_id %in% colnames(exprMatrix_corrected2))
exprMatrix_corrected_sub <- exprMatrix_corrected2[,df_tmp$final_sample_id] 
meta_data_sub <- meta_data2 %>% filter(SampleID %in% df_tmp$final_sample_id)
```

## PCA

Re-do PCA after batch correction

```{r}
# PCA after batch correction
pca_after <- prcomp(t(exprMatrix_corrected_sub), scale. = TRUE)
df_pca <- pca_after$x %>% as.data.frame() %>% 
  tibble::rownames_to_column(var = "SampleID") %>% 
  select(SampleID, PC1, PC2, PC3) %>% 
  rename(PC1_after = PC1,
         PC2_after = PC2,
         PC3_after = PC3) %>% 
  left_join(meta_data_sub)

# Calculate variance explained
pc_variances <- pca_after$sdev^2
variance_explained <- pc_variances / sum(pc_variances)
percent_variance_explained <- variance_explained * 100

# Extract variance for PC1 and PC2
pc1_variance <- percent_variance_explained[1]
pc2_variance <- percent_variance_explained[2]

# Create labels for plotting
pc1_labe_a <- paste0("PC1 (", round(pc1_variance, 2), "%)")
pc2_label_a <- paste0("PC2 (", round(pc2_variance, 2), "%)")


p2.1 <- 
ggplot(df_pca, aes(x = PC1_after, y = PC2_after, color = Batch)) +
  geom_point(size = pt_sz) +
  labs(
    title = "PCA After Limma Batch Correction",
    x = pc1_labe_a,
    y = pc2_label_a) +
  theme_bw() +
  theme_custom +
  theme(legend.position = "bottom") +
  scale_colour_manual(values = pal.study) +
  NULL

p2.2 <- 
ggplot(df_pca, aes(x = PC1_after, y = PC2_after, color = cohort)) +
  geom_point(size = pt_sz) +
  labs(
    title = "PCA After Limma Batch Correction",
    x = pc1_labe_a,
    y = pc2_label_a) +
  theme_bw() +
  theme_custom +
  theme(legend.position = "bottom") +
  scale_colour_manual(values = pal.cohort.n2) +
  NULL
```

Show all PCA plot

```{r}
p_all <- p1.1 + p1.2 + p2.1 + p2.2 + plot_layout(ncol=2)
p_all
ggsave(filename=file.path(wd$outCurr, "PCA_beforeAfter.pdf"), device="pdf", width=9, height=8)
```

## Keeping all proteins

In the batch correction strategy above, we limited our analysis to the common proteins (intersection proteins). Now we attempt to keep all the genes but also batch correct the data. We will reuse the correlation estimate and apply it to the individual expression. 


```{r}
pad_matrix <- function(mat, gene_list) {
  mat_df <- as.data.frame(mat)
  rownames(mat_df) <- mat_df[[1]]             # Set rownames using OlinkID column
  mat_df <- mat_df[,-1]                       # Drop OlinkID column
  mat_full <- matrix(NA, nrow = length(gene_list), ncol = ncol(mat_df))
  rownames(mat_full) <- gene_list
  colnames(mat_full) <- colnames(mat_df)
  shared_genes <- intersect(rownames(mat_df), gene_list)
  mat_full[shared_genes, ] <- as.matrix(mat_df[shared_genes, ])
  return(mat_full)
}

# 1. Create union of genes
all_genes <- Reduce(union, list(
  mat_olink1$OlinkID,
  mat_olink2$OlinkID,
  mat_olink3$OlinkID
))

# 2. Pad each batch
m1_full <- pad_matrix(mat_olink1, all_genes)
m2_full <- pad_matrix(mat_olink2, all_genes)
m3_full <- pad_matrix(mat_olink3, all_genes)

# 3. Combine all batches
m_all <- cbind(m1_full, m2_full, m3_full)

# 4. Subset metadata to match columns
meta_data <- meta_data[match(colnames(m_all), meta_data$SampleID), ]

# 5. Apply batch correction to full matrix (using the previosly computed corfit)
m_all_corrected <- removeBatchEffect(
  m_all,
  batch = meta_data$Batch,
  block = meta_data$ReplicateID,
  correlation = corfit$consensus
)
```

### Asses PCA

Lets use this version to check the PCA. Again, we need to remove technical duplicates


```{r}
# Important - select the variable after removing the poor correlations
df_tmp <- df_unq %>% filter(final_sample_id %in% colnames(m_all_corrected))
m_all_corrected_sub <- m_all_corrected[,df_tmp$final_sample_id] 
expr_corrc_allGenes <- m_all_corrected_sub
```



```{r}
# PCA after batch correction
mat_in <- m_all_corrected_sub[complete.cases(m_all_corrected_sub), ]
pca_after <- prcomp(t(mat_in), scale. = TRUE)
df_pca <- pca_after$x %>% as.data.frame() %>% 
  tibble::rownames_to_column(var = "SampleID") %>% 
  select(SampleID, PC1, PC2, PC3) %>% 
  rename(PC1_after = PC1,
         PC2_after = PC2,
         PC3_after = PC3) %>% 
  left_join(meta_data_sub)

# Calculate variance explained
pc_variances <- pca_after$sdev^2
variance_explained <- pc_variances / sum(pc_variances)
percent_variance_explained <- variance_explained * 100

# Extract variance for PC1 and PC2
pc1_variance <- percent_variance_explained[1]
pc2_variance <- percent_variance_explained[2]

# Create labels for plotting
pc1_labe_a <- paste0("PC1 (", round(pc1_variance, 2), "%)")
pc2_label_a <- paste0("PC2 (", round(pc2_variance, 2), "%)")


p3.1 <- 
ggplot(df_pca, aes(x = PC1_after, y = PC2_after, color = Batch)) +
  geom_point(size = pt_sz) +
  labs(
    title = "PCA After Limma Batch Correction",
    x = pc1_labe_a,
    y = pc2_label_a) +
  theme_bw() +
  theme_custom +
  theme(legend.position = "bottom") +
  scale_colour_manual(values = pal.study) +
  NULL

p3.2 <- 
ggplot(df_pca, aes(x = PC1_after, y = PC2_after, color = cohort)) +
  geom_point(size = pt_sz) +
  labs(
    title = "PCA After Limma Batch Correction",
    x = pc1_labe_a,
    y = pc2_label_a) +
  theme_bw() +
  theme_custom +
  theme(legend.position = "bottom") +
  scale_colour_manual(values = pal.cohort.n2) +
  NULL
```

# Obj 3: Perform bridging

> TL/DR - This step gives sub-optimal batch correction. The PCA does NOT look like its been integrated well between batches. So, no need to do this analysis. Scritps is archived in github

> We tried two methods to bridge the data; 1) The Olink default method, 2) Limma batch correction. 
It seems the Limma methods works best so we will use it

# Obj 4: Sample number

After filtering the samples, we want to get the sample numbers. We know the `ncol(exprMatrix_corrected_sub)` is the number of samples that were retained. So lets breakdown the samples

Generate a data frame of these samples that were retained

```{r}
df_meta_f5 <- df_unq %>% filter(final_sample_id %in% colnames(exprMatrix_corrected2))
nrow(df_meta_f5)
```

## Bar plot

Make a simple barplot to summarize the data. Two columns per cohort. Number of patients & number of samples. 

```{r}
# Calculate overlap between B and C_D cohorts
overlap_patients <- df_meta_f5 %>%
  filter(cohort %in% c("B", "C")) %>%
  group_by(mrn) %>%
  filter(n_distinct(cohort) > 1) %>%
  pull(mrn) %>%
  unique()

# Create summary data frame
summary_data <- df_meta_f5 %>%
  group_by(cohort) %>%
  summarise(
    n_patients = n_distinct(mrn),
    n_samples = n()
  ) %>%
  pivot_longer(
    cols = c(n_patients, n_samples),
    names_to = "metric",
    values_to = "count"
  )

# Create the barplot
p_bar <- ggplot(summary_data, aes(x = cohort, y = count, fill = metric)) +
  geom_bar(stat = "identity", position = position_dodge(width = 0.7), width = 0.6) +
  # Add overlay for C_D cohort showing overlap (only for patients)
  geom_bar(
    data = filter(summary_data, cohort == "C" & metric == "n_patients"),
    aes(y = length(overlap_patients), fill = "Overlap with B"),
    stat = "identity",
    position = position_dodge(width = 0.7),
    width = 0.6,
    alpha = 0.5
  ) +
  scale_fill_manual(
    values = c(
      "n_patients" = "#faedcd",
      "n_samples" = "grey60",
      "Overlap with B" = "darkred"
    ),
    labels = c("Number of Patients", "Number of Samples", "Overlap with B")
  ) +
  labs(
    x = "Cohort",
    y = "Count",
    fill = "Metric"
  ) +
  theme_bw() +
  theme(
    legend.position = "right",
    panel.grid.major.x = element_blank(),
    panel.grid.minor = element_blank()
  ) +
  geom_text(
    aes(label = count),
    position = position_dodge(width = 0.7),
    vjust = -0.5,
    size = 4
  ) +
  # Add annotation for overlap
  annotate(
    "text",
    x = "C_D",
    y = length(overlap_patients),
    label = paste("Overlap:", length(overlap_patients)),
    vjust = -0.5,
    size = 4,
    color = "darkred"
  )

ggsave(p_bar, file=file.path(wd$outCurr, "Patients_number_bar.pdf"), height = 5, width = 7)
```

## Swimmers plot

```{r}
df_meta_in <- df_meta_f5 %>% 
  select(mrn, HCI_cID, date, cohort, txt_stat, time2txt)

# 3. Relabel txt_stat as Pre1, Pre2, Post1, etc. by date within each patient/cohort/status
df_meta_in <- df_meta_in %>%
  group_by(mrn, cohort, txt_stat) %>%
  arrange(date, .by_group = TRUE) %>%
  mutate(
    txt_stat_ordered = paste0(txt_stat, row_number())
  ) %>%
  ungroup()

# 4. Assign pattern to each sample
patient_patterns <- get_patient_pattern(df_meta_in %>% mutate(txt_stat = txt_stat_ordered))
df_meta_in <- df_meta_in %>%
  left_join(patient_patterns %>% select(mrn, pattern, total_samples), by = "mrn")

# 5. Order patients by pattern and total_samples
ordered_mrns <- df_meta_in %>%
  distinct(mrn, pattern, total_samples) %>%
  arrange(
    factor(pattern, levels = pattern_order2),
    desc(total_samples)
  ) %>%
  pull(mrn)

# 6. Prepare swimmers_data with correct offsets and positions
swimmers_data <- df_meta_in %>%
  mutate(
    base_pos = case_when(
      cohort == "A" ~ 0,
      cohort == "B" ~ 1,
      cohort == "C" ~ 2
    ),
    # Extract trailing number from txt_stat_ordered, default to 1 if missing
    sample_num = as.numeric(str_extract(txt_stat_ordered, "\\d+$")),
    sample_num = ifelse(is.na(sample_num), 1, sample_num),
    # Offset for multiple samples of the same type
    x_offset = (sample_num - 1) * 0.2,
    # Treatment offset: Pre left, Post right
    treatment_offset = case_when(
      grepl("^Pre", txt_stat_ordered) ~ -0.2,
      grepl("^Post", txt_stat_ordered) ~ 0.2,
      TRUE ~ 0
    ),
    # Final x position, shifted to avoid 1/2 dashed line
    x_pos = base_pos + treatment_offset + x_offset - 0.05,
    # y position based on patient order
    y_pos = match(mrn, ordered_mrns),
    # For labeling
    sample_label = txt_stat_ordered
  )

# 7. Rectangle data for cohort boxes
rect_data <- data.frame(
  cohort = c("A", "B", "C"),
  xmin = c(-0.5, 0.5, 1.5),
  xmax = c(0.5, 1.5, 2.5)
)

# 8. Swimmers plot with time2txt above each point
p_swimmers_ordered <- ggplot(swimmers_data, aes(x = x_pos, y = y_pos)) +
  geom_rect(
    data = rect_data,
    inherit.aes = FALSE,
    aes(xmin = xmin, xmax = xmax, ymin = 0, ymax = max(swimmers_data$y_pos) + 1),
    fill = "grey95",
    alpha = 0.5
  ) +
  geom_vline(
    xintercept = c(0, 1, 2),
    linetype = "dashed",
    color = "grey50",
    alpha = 0.5
  ) +
  geom_point(
    aes(color = txt_stat_ordered),
    size = 3,
    shape = 16
  ) +
  # geom_text(
  #   aes(label = time2txt),
  #   vjust = -1,
  #   size = 3
  # ) +
  geom_line(
    aes(group = mrn),
    color = "grey50",
    alpha = 0.5
  ) +
  scale_x_continuous(
    breaks = 0:2,
    labels = c("Cohort A", "Cohort B", "Cohort C/D"),
    limits = c(-0.5, 2.5)
  ) +
  scale_color_manual(
    values = c(
      "Pre1" = "steelblue", "Pre2" = "steelblue", "Pre3" = "red",
      "Post1" = "orange", "Post2" = "orange", "Post3" = "orange"
    )
    #labels = c("Pre1", "Pre2", "Pre3", "Post1", "Post2", "Post3")
  ) +
  labs(
    x = "",
    y = "Patient",
    color = "Sample Type"
  ) +
  theme_bw() +
  theme(
    panel.grid.major.y = element_blank(),
    panel.grid.minor = element_blank(),
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank(),
    legend.position = "right"
  )

print(p_swimmers_ordered)

# 9. Save the plot
ggsave(
  p_swimmers_ordered,
  file = file.path(wd$outCurr, "Sample_collection_timeline_ordered.pdf"),
  height = max(10, length(unique(swimmers_data$mrn)) * 0.3),
  width = 8,
  limitsize = FALSE
)
write.csv(swimmers_data, file.path(wd$outCurr, "SwimmersData.csv"))


# 11. Zommed section
p_swimmers_ordered3 <- ggplot(
  swimmers_data %>% filter(!pattern %in% c("A", "B_Pre_Only", "B_Post_Only",
                                           "C_Pre_Only", "C_Post_Only")), 
  aes(x = x_pos, y = y_pos)) +
  geom_vline(
    xintercept = c(0, 1, 2),
    linetype = "dashed",
    color = "grey50",
    alpha = 0.5
  ) +
  geom_point(
    aes(color = txt_stat_ordered),
    size = 3,
    shape = 16
  ) +
  geom_line(
    aes(group = mrn),
    color = "grey50",
    alpha = 0.5
  ) +
  geom_text(
    aes(label = time2txt),
    vjust = -1,
    size = 3
  ) +
  scale_x_continuous(
    breaks = 0:2,
    labels = c("Cohort A", "Cohort B", "Cohort C/D"),
    limits = c(-0.5, 2.5)
  ) +
  scale_color_manual(
    values = c(
      "Pre1" = "steelblue", "Pre2" = "steelblue", "Pre3" = "red",
      "Post1" = "orange", "Post2" = "orange", "Post3" = "orange"
    )
    #labels = c("Pre1", "Pre2", "Pre3", "Post1", "Post2", "Post3")
  ) +
  labs(
    x = "",
    y = "Patient",
    color = "Sample Type"
  ) +
  theme_bw() +
  theme(
    panel.grid.major.y = element_blank(),
    panel.grid.minor = element_blank(),
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank(),
    legend.position = "right"
  )

ggsave(
  p_swimmers_ordered3,
  file = file.path(wd$outCurr, "Sample_collection_timeline_ordered_noTime_Zommed.pdf"),
  height = 15,
  width = 8,
  limitsize = FALSE
)

print(p_swimmers_ordered3)
```



## Sample number 

Lets get the sample numbers

```{r}
df_meta_in <- df_meta_f5
# For Cohort B
n_pre_b_samples <- df_meta_in %>% filter(cohort == "B", txt_stat %in% c("Pre", "Pre1", "Pre2", "Pre3")) %>% pull(HCI_cID) %>% unique() %>% length()
n_post_b_samples <- df_meta_in %>% filter(cohort == "B", txt_stat %in% c("Post", "Post1", "Post2", "Post3")) %>% pull(HCI_cID) %>% unique() %>% length()
n_pre_b_patients <- df_meta_in %>% filter(cohort == "B", txt_stat %in% c("Pre", "Pre1", "Pre2", "Pre3")) %>% pull(mrn) %>% unique() %>% length()
n_post_b_patients <- df_meta_in %>% filter(cohort == "B", txt_stat %in% c("Post", "Post1", "Post2", "Post3")) %>% pull(mrn) %>% unique() %>% length()

# For Cohort C
n_pre_c_samples <- df_meta_in %>% filter(cohort == "C", txt_stat %in% c("Pre", "Pre1", "Pre2", "Pre3")) %>% pull(HCI_cID) %>% unique() %>% length()
n_post_c_samples <- df_meta_in %>% filter(cohort == "C", txt_stat %in% c("Post", "Post1", "Post2", "Post3")) %>% pull(HCI_cID) %>% unique() %>% length()
n_pre_c_patients <- df_meta_in %>% filter(cohort == "C", txt_stat %in% c("Pre", "Pre1", "Pre2", "Pre3")) %>% pull(mrn) %>% unique() %>% length()
n_post_c_patients <- df_meta_in %>% filter(cohort == "C", txt_stat %in% c("Post", "Post1", "Post2", "Post3")) %>% pull(mrn) %>% unique() %>% length()

cat("B cohort:\n")
cat("  Pre samples:", n_pre_b_samples, "\n")
cat("  Post samples:", n_post_b_samples, "\n")
cat("  Pre patients:", n_pre_b_patients, "\n")
cat("  Post patients:", n_post_b_patients, "\n\n")

cat("C cohort:\n")
cat("  Pre samples:", n_pre_c_samples, "\n")
cat("  Post samples:", n_post_c_samples, "\n")
cat("  Pre patients:", n_pre_c_patients, "\n")
cat("  Post patients:", n_post_c_patients, "\n")
```


# Save objects

```{r}
save(
  exprMatrix_corrected2, exprMatrix_corrected_sub, 
  expr_corrc_allGenes, df_final_ids, df_meta_f5,
  file = file.path(wd$outData, "02_data.Rdata"))
```

